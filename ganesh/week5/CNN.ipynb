{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bab7e6a-56db-45b1-b522-37d9aebf3075",
   "metadata": {},
   "source": [
    "**CNN: Convolutional Neural Network(CNN or ConvNet)** is a class of deep neural networks which is mostly used to do image recognition, image classification, object detection, etc.\n",
    "\n",
    "![CNN](CNN-deep-learning-architecture.png)\n",
    "\n",
    "**Google** uses it for photo search, **Facebook** for their automatic tagging algorithms, **Amazon** for their product recommendations, and the list goes on and onâ€¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b255b1ae-085c-4cef-a876-d89112990ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2525e61-11ad-4301-b304-2bb3dee70424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f33cf4c7-4c2d-4d58-b1b9-f332b476e71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29d41255-2516-4112-acb9-203466afa5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb0244f-0ff9-43a3-9680-b4e3396d200e",
   "metadata": {},
   "source": [
    "### Part-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907077df-2148-4492-97ba-151930a2d6c7",
   "metadata": {},
   "source": [
    "### Data Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8340175f-eae0-491c-aa5f-1f0ed117be53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                  shear_range = 0.2,\n",
    "                                  zoom_range = 0.2,\n",
    "                                  horizontal_flip = True)\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        r'C:\\Users\\ganig\\OneDrive\\Desktop\\Dataset\\training_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80764c1d-bba2-4777-8076-93a24ebcdc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        r'C:\\Users\\ganig\\OneDrive\\Desktop\\Dataset\\test_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60631c3b-94d6-4cbb-821d-3e72a8799166",
   "metadata": {},
   "source": [
    "### Part-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aa77e5-4dec-4e3c-98e1-e3ed80a16bb1",
   "metadata": {},
   "source": [
    "**Building CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3df140bf-f8ac-4164-b67f-092b3abda89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn=tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa0fc90-6df7-46a7-8601-f880c6a82dd6",
   "metadata": {},
   "source": [
    "**step-1 convolution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ddd662a0-af32-46c2-b6f2-3cf5ca6b4412",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation=\"relu\",input_shape=[64,64,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebcb437-2c64-4860-8f42-f4c74388686e",
   "metadata": {},
   "source": [
    "**step-2 max pooling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c77a5cb5-0c6f-4f9f-9228-518048329205",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff59906-4846-45de-b1e2-c16b7d358e91",
   "metadata": {},
   "source": [
    "**Adding second convolution layer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "711f1117-e72b-49eb-8e08-3e5c10770656",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation=\"relu\"))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fbf00641-25c2-430d-bf89-b8693a84621e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_5 (Conv2D)           (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 31, 31, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 29, 29, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 14, 14, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10144 (39.62 KB)\n",
      "Trainable params: 10144 (39.62 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf3fb1c-dbdf-49d7-83b7-da24ea9585c6",
   "metadata": {},
   "source": [
    "**step-3 Flattening**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5c9e5001-398a-4ab0-8959-064ce7d55cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f79afb-28d9-4452-9cf0-51940cf4f654",
   "metadata": {},
   "source": [
    "**step-4 Full connection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "478d8584-f1d6-4bc0-be69-b34cca8394b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128,activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31275dee-9ec8-4bdd-8ee2-d9725e2a4099",
   "metadata": {},
   "source": [
    "**step-5 Output layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a5da1793-adfe-4544-a8b8-9b94293812e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1,activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390ae866-887e-4c16-a5b2-7300cd31f830",
   "metadata": {},
   "source": [
    "### Part-3 Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277ffeb2-3a3b-4413-bfcf-d4e2a1a2c96a",
   "metadata": {},
   "source": [
    "**compiling the cnn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ec9d20b5-ce21-42cd-ab50-02408ca4bc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb9ff65-c9a0-4cbd-987c-d0d243cd5321",
   "metadata": {},
   "source": [
    "**Training the CNN on the training set and evaluating it on the test set:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f2e106a5-7869-4fbd-a929-c12f9d65c7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 33s 127ms/step - loss: 0.6651 - accuracy: 0.5986 - val_loss: 0.5828 - val_accuracy: 0.6980\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 34s 137ms/step - loss: 0.5949 - accuracy: 0.6836 - val_loss: 0.5378 - val_accuracy: 0.7270\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 35s 142ms/step - loss: 0.5515 - accuracy: 0.7218 - val_loss: 0.5249 - val_accuracy: 0.7415\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 33s 133ms/step - loss: 0.5234 - accuracy: 0.7364 - val_loss: 0.5324 - val_accuracy: 0.7410\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 35s 140ms/step - loss: 0.4886 - accuracy: 0.7646 - val_loss: 0.4757 - val_accuracy: 0.7715\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 41s 163ms/step - loss: 0.4709 - accuracy: 0.7726 - val_loss: 0.4821 - val_accuracy: 0.7730\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 35s 139ms/step - loss: 0.4566 - accuracy: 0.7830 - val_loss: 0.5171 - val_accuracy: 0.7575\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 33s 133ms/step - loss: 0.4337 - accuracy: 0.7943 - val_loss: 0.4521 - val_accuracy: 0.7930\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 0.4183 - accuracy: 0.8085 - val_loss: 0.4737 - val_accuracy: 0.7910\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 0.4064 - accuracy: 0.8139 - val_loss: 0.4857 - val_accuracy: 0.7875\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 0.3873 - accuracy: 0.8242 - val_loss: 0.4849 - val_accuracy: 0.7935\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 43s 174ms/step - loss: 0.3736 - accuracy: 0.8295 - val_loss: 0.4425 - val_accuracy: 0.8040\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 44s 175ms/step - loss: 0.3658 - accuracy: 0.8353 - val_loss: 0.4286 - val_accuracy: 0.8170\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 47s 187ms/step - loss: 0.3527 - accuracy: 0.8447 - val_loss: 0.4714 - val_accuracy: 0.7950\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 0.3362 - accuracy: 0.8480 - val_loss: 0.4446 - val_accuracy: 0.8075\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 35s 139ms/step - loss: 0.3311 - accuracy: 0.8550 - val_loss: 0.4675 - val_accuracy: 0.8060\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 44s 175ms/step - loss: 0.3088 - accuracy: 0.8651 - val_loss: 0.4530 - val_accuracy: 0.8230\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 37s 147ms/step - loss: 0.3090 - accuracy: 0.8661 - val_loss: 0.4701 - val_accuracy: 0.8080\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 36s 143ms/step - loss: 0.2784 - accuracy: 0.8794 - val_loss: 0.5168 - val_accuracy: 0.7955\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 0.2722 - accuracy: 0.8834 - val_loss: 0.4982 - val_accuracy: 0.8080\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 34s 134ms/step - loss: 0.2601 - accuracy: 0.8911 - val_loss: 0.5295 - val_accuracy: 0.8045\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 33s 133ms/step - loss: 0.2543 - accuracy: 0.8939 - val_loss: 0.4961 - val_accuracy: 0.8085\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 33s 133ms/step - loss: 0.2324 - accuracy: 0.9053 - val_loss: 0.5002 - val_accuracy: 0.8190\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 40s 160ms/step - loss: 0.2145 - accuracy: 0.9115 - val_loss: 0.5010 - val_accuracy: 0.8275\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 0.2163 - accuracy: 0.9105 - val_loss: 0.5776 - val_accuracy: 0.8075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1b57c326550>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x=training_set,validation_data=test_set,epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53ba548-c5c6-46a5-aa65-463db2380b6a",
   "metadata": {},
   "source": [
    "**Part-4 Making single prediction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "52137a5f-6266-4d79-a5dd-78636552b8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 121ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "test_image= image.load_img(r\"C:\\Users\\ganig\\OneDrive\\Desktop\\Dataset\\single_prediction\\Cat or dog_1.jpg\",target_size=(64,64))\n",
    "test_image=image.img_to_array(test_image)\n",
    "test_image=np.expand_dims(test_image,axis=0)\n",
    "result=cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0]==1:\n",
    "    prediction = \"dog\"\n",
    "else:\n",
    "    prediction = \"cat\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "75e1ddb7-d797-448a-9122-769fe5b0d957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64c26de-c156-499e-a68e-7b4bf25cbbdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
